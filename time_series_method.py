# -*- coding: utf-8 -*-
"""time series method.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-M8a41ZRwC9vQJ-SCa2sRvKas3JXpvx9

### Import Library
"""

import warnings
import itertools

import pandas as pd
import numpy as np

import statsmodels.api as sm
import statsmodels.tsa.api as smt
import statsmodels.formula.api as smf

import matplotlib.pyplot as plt

features = pd.read_csv('D:\Decion Making\data\Features data set.csv')
train = pd.read_csv('D:\Decion Making\data\Train.csv')
stores = pd.read_csv('data\stores data-set.csv')
test = pd.read_csv('D:\Decion Making\data\Test.csv')

train.info()

test.info()

# changing datatype to datetime format
train['Date']=pd.to_datetime(train['Date'])
test['Date']=pd.to_datetime(test['Date'])

# Data Exploratory
train['Month']=train['Date'].dt.month
train['Year']=train['Date'].dt.year
train['Dayofweek']=train['Date'].dt.dayofweek

train.head()

# data Exploratory
test['Month']=test['Date'].dt.month
test['Year']=test['Date'].dt.year
test['Dayofweek']=test['Date'].dt.dayofweek

test.head()

# set the dates as the index of the dataframe, so that it can be treated as a time-series dataframe
train.set_index('Date',inplace=True)
test.set_index('Date',inplace=True)

# extract out the time-series
sales=train.groupby('Date')['Weekly_Sales'].sum()
sales.head()

"""## Seasonality - Time Series Decomposition
The results represent a pivot table(bảng chéo) which has values that specified as "Weekly Sales" 
and the columns that are determined by the "Year" column in the Dataframe,
and the rows are determined by the "Month" column
"""

monthly_sales = pd.pivot_table(train,values="Weekly_Sales", columns="Year",index="Month")
monthly_sales=monthly_sales.reindex(index=[1,2,3,4,5,6,7,8,9,10,11,12])
monthly_sales

monthly_sales.plot()

"""We relized that June, November, December are the peak ( đỉnh cao) months for weekly sales and can see a seasonal cycle of 12 months where the mean value of each month starts with a decreasing trend in the beginning of the year and increases towards the end of the year. We can see a seasonal effect with a cycle of 12 months.

## Irregular Remainder ( phần dư bất thường)
"""

decomposition = sm.tsa.seasonal_decompose(sales, extrapolate_trend=8)

fig = decomposition.plot()
fig.set_figwidth(12)
fig.set_figheight(8)
fig.suptitle('Decomposition of additive time series')
plt.show()

decomp_output = pd.DataFrame(pd.concat([decomposition.observed, decomposition.trend, decomposition.seasonal, decomposition.resid], axis=1))

decomp_output.columns = ['observed', 'Trend', 'Seasonal', 'Irregular']

decomp_output

"""## Stationary Data for ARIMA models"""

from statsmodels.tsa.stattools import adfuller
adfuller(sales)

# Perform Dickey-Fuller test:
from statsmodels.tsa.stattools import adfuller
print('Results of Dickey-Fuller Test:')
dftest = adfuller(sales)
dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#lags Used', 'Number of Observations Used'])
for key, value in dftest[4].items():
    dfoutput['Critical Value (%s)'%key] = value
print(dfoutput)

plt.plot(sales)

sales_log = np.log10(sales)

plt.plot(sales_log)

# Perform Dickey-Fuller test:
from statsmodels.tsa.stattools import adfuller
print('Results of Dickey-Fuller Test:')
dftest = adfuller(sales_log)
dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#lags Used', 'Number of Observations Used'])
for key, value in dftest[4].items():
    dfoutput['Critical Value (%s)'%key] = value
print(dfoutput)

"""Analysis the result of Dickey Fuller Test 
+ Test Statistic: -5.908297957186336
+ p-value 2.675979158986003e-07 (very small) (This is a value to test stationarity of the time series. If the p-value is smaller than a predefined threshold (usyally 0.05) )

Test Statistic is compared with critical value to check the stationarity of the time series. If Test Statistic less than Critical Value, the time series is stable. With this time series, it is stable.

"""

sales.diff(1).head()

sales_diff1 = sales.diff(1)
sales_diff1.dropna(inplace=True)

plt.plot(sales_diff1)

# Perform Dickey-Fuller test:
from statsmodels.tsa.stattools import adfuller
print('Results of Dickey-Fuller Test:')
dftest = adfuller(sales_diff1)
dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#lags Used', 'Number of Observations Used'])
for key, value in dftest[4].items():
    dfoutput['Critical Value (%s)'%key] = value
print(dfoutput)

plt.plot(sales_diff1)

"""The following are some of my key observations from this analysis:

1, Trend: 12 months moving average look quite similar to a straitght line so we could have easly used linear regression to estimate the trend in this data

2, Seasonality: As discussed, seasonal plot displays a fairly consistent month-on-month pattern. ( nhất quán theo tháng)

$Seasonality_{t} ×Remainder_{t}=Y_{t}-Trend_{t}$


​

"""

fig, axes = plt.subplots(1, 2)
fig.set_figwidth(18)
fig.set_figheight(6)
smt.graphics.plot_acf(sales_diff1, lags=30, ax=axes[0], alpha=0.5)
smt.graphics.plot_pacf(sales_diff1, lags=30, ax=axes[1], alpha=0.5,method='ols')
plt.tight_layout()

model = sm.tsa.statespace.SARIMAX(sales_diff1, order = (1,0,1), seasonal_order= (1,0,0,12),
                                             enforce_stationarity=False,
                                             enforce_invertibility=False)

results = model.fit()
print(results.summary())

"""#### Iterate the process to find the best values for p, d, q and P, D, Q"""

# Define the p, d and q parameters to take any value between 0 and 2
p = q = range(0, 2)
d = range(0,2)
# Generate all different combinations of p, d and q triplets
pdq = list(itertools.product(p, d, q))

pdq

# Generate all different combinations of seasonal p, q and q triplets
D = range(0,2)
P = Q = range(0, 2) 
seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(P, D, Q))]
seasonal_pdq

import sys
warnings.filterwarnings("ignore") # specify to ignore warning messages

best_aic = np.inf
best_pdq = None
best_seasonal_pdq = None
temp_model = None

for param in pdq:
    for param_seasonal in seasonal_pdq:
        
        try:
            temp_model = sm.tsa.statespace.SARIMAX(sales_log,
                                             order = param,
                                             seasonal_order = param_seasonal,
                                             enforce_stationarity=False,
                                             enforce_invertibility=False)
            results = temp_model.fit()

            # print("SARIMAX{}x{}12 - AIC:{}".format(param, param_seasonal, results.aic))
            if results.aic < best_aic:
                best_aic = results.aic
                best_pdq = param
                best_seasonal_pdq = param_seasonal
        except:
            #print("Unexpected error:", sys.exc_info()[0])
            continue
print("Best SARIMAX {} x {} 12 model - AIC:{}".format(best_pdq, best_seasonal_pdq, best_aic))

"""### Predict sales using the best fit ARIMA model"""

best_model=sm.tsa.statespace.SARIMAX(sales_log,order=(1, 1, 1),seasonal_order=(0, 0, 0, 12) ,enforce_stationarity=False,
                                      enforce_invertibility=False)
best_results=best_model.fit()

print(best_results.summary().tables[0])
print(best_results.summary().tables[1])

sales

pred_dynamic = best_results.get_prediction(start=pd.to_datetime('2012-09-14'), full_results=True)

pred_dynamic.predicted_mean

forecast=np.power(10, pred_dynamic.predicted_mean)

actual = sales['2012-09-14':]
actual

#Mean absolute percentage error
MAPE_last1year = np.mean(np.abs(actual - forecast)/actual)
MAPE_last1year

print('The Mean Absolute percentage error of our forecast for last two years is {}'.format(round(MAPE_last1year, 4)))

# Compute the mean square error 
mse= ((actual-forecast)** 2).mean()
print('The Mean Squared Error of our forecasts is {}'.format(round(mse,4)))
print("The Root Mean Squared Error is {}".format(np.sqrt(round(mse,4))))

np.power(10, best_results.forecast(steps=12))

"""### Forecast sales using the best fit ARIMA model
The next step is to predict weekly sales for next 3 years i.e. for 2013, 2014, and 2015 through the above model.

"""

n_steps = 36
# Get forecast 36 steps (3 years) ahead in future

pred_uc_99 = best_results.get_forecast(steps=36, alpha=0.01) # alpha=0.01 signifies 99% confidence interval
pred_uc_95 = best_results.get_forecast(steps=36, alpha=0.05) # alpha=0.05 95% CI

# Get confidence intervals (khoảng tin cậy) 95% & 99% of the forecasts
pred_ci_99 = pred_uc_99.conf_int()
pred_ci_95 = pred_uc_95.conf_int()

pred_ci_99.head()

pred_ci_95.head()

n_steps=36
idx=pd.date_range(sales.index[-1],periods=n_steps,freq='MS')
# This function create a series which have begin date =[date]-1 -  the final date in the time series, 
# kéo dài n_steps, frequences = MS 
fc_95 = pd.DataFrame(np.column_stack([np.power(10, pred_uc_95.predicted_mean), np.power(10, pred_ci_95)]), 
                     index=idx, columns=['forecast', 'lower_ci_95', 'upper_ci_95'])
fc_99 = pd.DataFrame(np.column_stack([np.power(10, pred_ci_99)]), 
                     index=idx, columns=['lower_ci_99', 'upper_ci_99'])

fc_all = fc_95.combine_first(fc_99)
fc_all = fc_all[['forecast', 'lower_ci_95', 'upper_ci_95', 'lower_ci_99', 'upper_ci_99']] # just reordering columns
fc_all.head() # ket qua du bao

pd.plotting.register_matplotlib_converters()

# plot the forecast along with the confidence band
axis = sales.plot(label='Observed', figsize=(15, 6))
fc_all['forecast'].plot(ax=axis, label='Forecast', alpha=0.7)
#axis.fill_between(fc_all.index, fc_all['lower_ci_95'], fc_all['upper_ci_95'], color='k', alpha=.25)
axis.fill_between(fc_all.index, fc_all['lower_ci_99'], fc_all['upper_ci_99'], color='k', alpha=.25)
axis.set_xlabel('Years')
axis.set_ylabel('Weekly Sales')
plt.legend(loc='best')
plt.show()
best_results.plot_diagnostics(lags=30, figsize=(16,12))
plt.show()